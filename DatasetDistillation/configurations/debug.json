{
  "distill-epochs": 5,
  "epochs": 400,
  "distilled-batches": 2,
  "n-classes": 10,
  "examples-per-class": 2,
  "distill-lr": 0.01,
  "alpha": 0.1,
  "optimizer-name": "adagrad",
  "training-data-batch-size": 1024
}